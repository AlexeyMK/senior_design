Monday, Feb 6: ran first experiment
MTurk ID 2I63W5XH0JPPYT85BMGLVP75SV1KEG
Cost: $5.50 max, about half of that expected.

Conditions: none to speak of really, just running an experiment to get started and make sure that everything is ironed out. Haven't written the code to pay out bonuses and accept, or to automate the experiment process - that'll come next.

Result: A couple of turkers were able to get in double-dips. Learned how to fix that, will implement next, as well as looking at the ratings and correlation between accepting/rejecting and amt offered, and review/amt offered.

Analysis: Mean squared error (though we'll need to adjust moving forward) was 0.99, and the resulting chart looks is in results/
Also, I'm currently throwing away did-not-reviews, which does not seem like a useful analysis trick. Hopefully I'll find a better alternative moving forward.

Monday, Feb 27: running second experiment
Name (rather than ID): base_case_2 
MTurk ID (just in case): 2PT727M0IGFKNO7RBNJVI4VIWAAEZ4
Same as first, but lowered price to three cents
Not changing much, but the framework to quickly run and analyze experiments is now in place.
Cost: 3c * 10 + max of 10c * 5 * 10, so 5.30, expecting ~half.

Result: A couple of turkers emailed to complain that I hadn't paid them yet.  Glancing at the results, I see one turker rejecting offers as high as 5/10, so showing behavior very similar to real subjects. Very cool.

Analysis: Mean Squared error was 1.21, a slight increase. No idea why, though (difference = 3 cents base vs 5). Saving chart in results/.  Moving to a couple of alternate experiments to make sure we can run them in parralel.

Sunday, Mar 11: running first conditions
Name (rather than ID): guarantee_anonymous_1, guarantee_feedback_shown_1, thank_on_bad_review_1
Cost: ~$5 * 3 = $15

Conditions: trying three variations (will separate as needed) more than anything to validate that variations work

Results: saved accordingly.  Eyeballing, it doesn't seem like the RMSE differed signficantly across conditions. Everybody is paid

Sunday, Mar 26: running mixed conditions, 25 subjects each.  Tests are called
- thank_on_bad_review_mandatory_review_experimental_validity_reason_25subjects
- mandatory_review_personal_gain_25subjects

Going to start mixing things up now that the framework is set up and add more conditions.

Results: saved, looks like one of them had slightly lower error, but waiting for Jessica to get the measurement system set up. Had what I thought was an error as a result of two turkers' reports, but that doesn't look accurate anymore (double-checked, their stuff got submitted)

Monday, Mar 26: Started two new experiment sets in which we run hundreds of experiments but with only one subject each.  This (hopefully) lets us tease out specific condition attributes. Also trying new conditions for alternate rating systems (-2..2, this vs previous, upvote vs downvote, etc) as opposed to the traditional 1..5.
